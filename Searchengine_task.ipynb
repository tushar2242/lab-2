{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tUxCbOkvAUtp"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "#importing libraries for warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "#importing libraries for pandas\n",
    "import numpy as np\n",
    "#importing libraries for numpy\n",
    "import statsmodels.api as sm\n",
    "#importing libraries for statsmodels\n",
    "import scipy\n",
    "#importing libraries for scipy\n",
    "import scipy.stats as stats\n",
    "#importing libraries for scipy\n",
    "from matplotlib import pyplot as plt\n",
    "#importing libraries for matplotlib\n",
    "import seaborn  as sns\n",
    "#importing libraries for seaborn\n",
    "from plotly import express as px\n",
    "#importing libraries for seaborn\n",
    "import os\n",
    "#importing os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QR_veKmgU1-E",
    "outputId": "745fd9dc-fabd-4586-d954-f59cabc60c46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\A2G\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#importing nltk\n",
    "nltk.download('stopwords')\n",
    "#downloading stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Esvc2cGuU3bz",
    "outputId": "904ff37a-c318-44b4-a0d7-0f6df10fd02d"
   },
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrusiT1mVCRK",
    "outputId": "1a0cb981-4aa9-44f4-87ed-d56c3fc0f59f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\A2G\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#importing nltk\n",
    "nltk.download('punkt')\n",
    "#downloading punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DPhADrtjVGdc"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#importing libraries for stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "#importing libraries for PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#importing libraries for word_tokenize\n",
    "text1 = 'This is an example sentence.'\n",
    "#initialize texts\n",
    "stop_wordss = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "token1s = word_tokenize(text1.lower())\n",
    "filtered_tokens = [stemmer.stem(token) for token in token1s if token not in stop_wordss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "40V0SyonVJ27"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "text1 = 'This is an example sentence.'\n",
    "stop_words1 = set(stopwords.words('english'))\n",
    "#defining text\n",
    "stemmers = PorterStemmer()\n",
    "tokens = word_tokenize(text1.lower())\n",
    "filtered_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eD6tiM5KVMiZ",
    "outputId": "f6aab36f-7c97-4125-a2a4-19b362415c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whoosh in c:\\users\\a2g\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JoQu9eqcVPJj"
   },
   "outputs": [],
   "source": [
    "from whoosh import fields\n",
    "from whoosh.index import create_in\n",
    "#importing whoosh libraries\n",
    "schemas = fields.Schema(\n",
    "    title=fields.TEXT(stored=True),\n",
    "    authors=fields.KEYWORD(stored=True),\n",
    "    year=fields.NUMERIC(stored=True),\n",
    "    url=fields.ID(stored=True),\n",
    "    abstract=fields.TEXT(stored=True)\n",
    ")\n",
    "#intialize index\n",
    "index_dir = 'path/to/index/dir'\n",
    "if not os.path.exists(index_dir):\n",
    "    os.makedirs(index_dir)\n",
    "indexs = create_in(index_dir, schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XApDH9H9WYL5"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#importing libraries of bs4\n",
    "def extract_metadata(url):\n",
    "    \"\"\"\n",
    "    Extract metadata for a publication from its web page.\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    #defining variables \n",
    "    title = soup.find('h1', {'class': 'title'}).text.strip()\n",
    "    authors = [a.text.strip() for a in soup.find_all('a', {'class': 'person-name'})]\n",
    "    year = int(soup.find('span', {'class': 'year'}).text.strip())\n",
    "    abstract = soup.find('div', {'class': 'abstract'}).text.strip()\n",
    "    pureportal_url = soup.find('a', {'class': 'pure-link'})['href']\n",
    "    #defining metadata\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'year': year,\n",
    "        'url': url,\n",
    "        'abstract': abstract,\n",
    "        'pureportal_url': pureportal_url\n",
    "    }\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "esMq9F1YWoHo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from whoosh import fields\n",
    "from whoosh.index import create_in\n",
    "from whoosh.qparser import MultifieldParser\n",
    "\n",
    "# Defining the queries based on schema for the particular search index\n",
    "schema = fields.Schema(\n",
    "    title=fields.TEXT(stored=True),\n",
    "    authors=fields.KEYWORD(stored=True),\n",
    "    year=fields.NUMERIC(stored=True),\n",
    "    url=fields.ID(stored=True),\n",
    "    abstract=fields.TEXT(stored=True),\n",
    "    pureportal_url=fields.ID(stored=True)\n",
    ")\n",
    "# Creating of the taegeted search index \n",
    "index_dirs = 'path/to/index/dir'\n",
    "if not os.path.exists(index_dirs):\n",
    "    os.makedirs(index_dirs)\n",
    "index = create_in(index_dirs, schema)\n",
    "# Defining URL based on CSM publications\n",
    "base_urls = 'https://pureportal.coventry.ac.uk'\n",
    "# Defining URL based on CSM publications search page\n",
    "search_urls = 'https://www.coventry.ac.uk/research/areas-of-research/computational-science-mathematical-modelling/'\n",
    "# Defining the major number of particular pages to crawl\n",
    "max_pages = 5\n",
    "# Crawling based on the search results pages and additon of publications based on search index\n",
    "for page_num in range(1, max_pages + 1):\n",
    "    # Fetching data from the search results page\n",
    "    page_urls = search_urls.format(page_num)\n",
    "    page = requests.get(page_urls)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    #  publication links based on the url link\n",
    "    publication_links = soup.find_all('a', {'class': 'link-item'})\n",
    "    # Extracting metadata for each publication and add it to the search index\n",
    "    for link in publication_links:\n",
    "        publication_url = base_urls + link['href']\n",
    "        metadata = extract_metadata(publication_url)\n",
    "        with index.writer() as writer:\n",
    "            writer.add_document(title=metadata['title'],\n",
    "                                authors=metadata['authors'],\n",
    "                                year=metadata['year'],\n",
    "                                url=metadata['url'],\n",
    "                                abstract=metadata['abstract'],\n",
    "                                pureportal_url=metadata['pureportal_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kaeW6lt2Xs3Z"
   },
   "outputs": [],
   "source": [
    "def extract_metadata(url):\n",
    "    # exraction of the data from the publication details page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extracting targeted metadata from the following page\n",
    "    title = soup.find('h1', {'class': 'title'}).text.strip()\n",
    "    authors = [a.text.strip() for a in soup.find_all('span', {'itemprop': 'author'})]\n",
    "    year = int(soup.find('span', {'itemprop': 'datePublished'}).text.strip())\n",
    "    abstract = soup.find('div', {'class': 'row abstract'}).find('div', {'class': 'value'}).text.strip()\n",
    "    pureportal_url = url\n",
    "    # genarytion of the metadata for dictionary\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'year': year,\n",
    "        'url': url,\n",
    "        'abstract': abstract,\n",
    "        'pureportal_url': pureportal_url\n",
    "    }\n",
    "    #returning metadata\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v7xe49edYDH2"
   },
   "outputs": [],
   "source": [
    "# pip install metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "orkxJho2Xusf"
   },
   "outputs": [],
   "source": [
    "    # Extracting targeted metadata for individual publication and additon of the search index\n",
    "for link in publication_links:\n",
    "     publication_url = base_urls + link['href']\n",
    "     #defining metadata\n",
    "     metadata = extract_metadata(publication_url)\n",
    "     # Adding another metadata of search index\n",
    "     with index.writer() as writer:\n",
    "        writer.add_document(**metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E0AGprtqXwhe"
   },
   "outputs": [],
   "source": [
    "# extraction of metadata based on the publication with the details page\n",
    "def extract_metadata(url):\n",
    "    # Fetching values of the publication based ondetails page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extracting details of metadata from the page\n",
    "    title = soup.find('h1', {'class': 'title'}).text.strip()\n",
    "    authors = [a.text.strip() for a in soup.find_all('span', {'itemprop': 'author'})]\n",
    "    year = int(soup.find('span', {'itemprop': 'datePublished'}).text.strip())\n",
    "    abstract = soup.find('div', {'class': 'row abstract'}).find('div', {'class': 'value'}).text.strip()\n",
    "    pureportal_url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PhA_KNXrZ-Vt"
   },
   "outputs": [],
   "source": [
    "# Crawling based on the results pages and additon of the publications based on the search index\n",
    "for page_num in range(1, max_pages + 1):\n",
    "    page_url = search_urls.format(page_num)\n",
    "    page = requests.get(page_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    publication_links = soup.find_all('a', {'class': 'link-item'})\n",
    "    for link in publication_links:\n",
    "        publication_url = base_urls + link['https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=information+technology&btnG=&oq=informa']\n",
    "        metadata = extract_metadata(publication_url)\n",
    "        with index.writer() as writer:\n",
    "            writer.add_document(**metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lfzS5XZWBAl3"
   },
   "outputs": [],
   "source": [
    "# Define  parser query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NWOVsmmaN38",
    "outputId": "dc032230-bf4e-410e-8a6e-20ee19ccda1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results (0 total):\n"
     ]
    }
   ],
   "source": [
    "query_parsers = MultifieldParser(['title', 'authors', 'abstract'], schema=index.schema)\n",
    "\n",
    "query_strs = input('Enter your search query: ')\n",
    "\n",
    "query1 = query_parsers.parse(query_strs)\n",
    "with index.searcher() as searcher:\n",
    "    result1s = searcher.search(query1)\n",
    "    print(f'Search results ({len(result1s)} total):')\n",
    "    for i, result in enumerate(result1s):\n",
    "        print(f'{i + 1}. {result[\"title\"]} ({result[\"year\"]}) by {\", \".join(result[\"authors\"])}')\n",
    "        print(f'   Abstract: {result[\"abstract\"]}')\n",
    "        print(f'   URL: {result[\"url\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dcgctnCBIMF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
